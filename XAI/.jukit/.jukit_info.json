{"cmd": "\nc_vae = ClusterVAE(Config.num_classes).to(Config.device)\ncritic = Critic(Config.num_classes).to(Config.device)\n\nvae_optimizer = optim.Adam(c_vae.parameters(), lr=learning_rate)\ncritic_optimizer = optim.Adam(critic.parameters(), lr=learning_rate)\n\nmodel.eval()\nmodel.requires_grad_(False)  # Freeze main model parameters\n\n# c = torch.ones(batch_size, 1, Config.img_dim, Config.img_dim).to(Config.device)\n\nfor epoch in range(Config.epochs):\n    total_loss_vae = 0\n    total_loss_critic = 0\n    pbar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.epochs}\")\n    acc_list = []\n\n    for data, labels in pbar:\n        data = data.to(Config.device)\n        labels = labels.to(Config.device)\n\n        # train the VAE\n        c_vae.train()\n        critic.eval()\n\n        vae_optimizer.zero_grad()\n\n        with torch.no_grad():\n            x = model(data)\n            _, pred_base = x.max(1)\n            if torch.isnan(x).any():\n                print(\"NaN detected in model output x. Stopping training.\")\n                break\n            c = critic(x)\n            if torch.isnan(c).any():\n                print(\"NaN detected in critic output c. Stopping training.\")\n                break\n\n        output = c_vae(x)\n        loss_vae = loss_fn_vae(output[\"recon\"], x, output[\"z_mean\"], c, data)\n        total_loss_vae += loss_vae.item()\n\n        loss_vae.backward()\n        vae_optimizer.step()\n\n        # train the critic\n        c_vae.eval()\n        critic.train()\n\n        critic_optimizer.zero_grad()\n        # c: output of the critic from x (x was computed earlier using model(data))\n        c = critic(x)\n\n        # Detach the VAE output so that gradients do NOT flow into c_vae.\n        with torch.no_grad():\n            output = c_vae(x)\n\n        # Compute the critic loss using the detached z_mean from c_vae.\n        loss_critic = loss_fn_critic(x, output[\"z_mean\"], c, data, model)\n\n        loss_critic.backward()\n        torch.nn.utils.clip_grad_norm_(critic.parameters(), max_norm=1.0) # Gradient clipping\n        critic_optimizer.step()\n        total_loss_critic += loss_critic.item()\n\n        with torch.no_grad():\n            _, pred_z = model(c * data).max(1)\n            acc = (pred_base == pred_z).float().mean().item()\n            acc_list.append(acc)\n\n        pbar.set_postfix(loss=total_loss_critic / (pbar.n + 1))\n\n    print(\n        f\"Epoch {epoch+1}/{Config.epochs}, Loss VAE: {total_loss_vae/len(train_loader):.4f}, Loss Critic: {total_loss_critic/len(train_loader):.4f}, Acc: {sum(acc_list)/len(acc_list):.4f}\"\n    )\n", "cmd_opts": " --cell_id=tQqyQelokQ -s", "import_complete": 1, "terminal": "tmux"}