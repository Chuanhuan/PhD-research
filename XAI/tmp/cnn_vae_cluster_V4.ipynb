{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION: Define a CNN model for MNIST dataset and load the model weights\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the directory containing helper.py to the Python path\n",
    "sys.path.append(os.path.abspath(\"/home/jack/Documents/PhD-research/XAI\"))\n",
    "\n",
    "# Explicitly import the required functions from helper\n",
    "from helper import plot_recon_img, plot_patch_image\n",
    "\n",
    "# Other imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "import torch.distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Get the directory of the current file\n",
    "# current_file_directory = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Change the current working directory to the file's directory\n",
    "# os.chdir(current_file_directory)\n",
    "\n",
    "# Import other necessary modules\n",
    "from vae_model import *\n",
    "\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "trainset = MNIST(\n",
    "    root=\"~/Documents//data\", train=True, download=True, transform=transform\n",
    ")\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = MNIST(\n",
    "    root=\"~/Documents//data\", train=False, download=True, transform=transform\n",
    ")\n",
    "testloader = DataLoader(testset, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2921ac4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# SECTION: load MNIST dataa only 8\n",
    "\n",
    "\n",
    "# Create the dataset for digit 8\n",
    "testset_8 = MNIST_8(testset)\n",
    "testloader_8 = DataLoader(testset_8, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# Create the dataset for digit 9\n",
    "testset_9 = MNIST_9(testset)\n",
    "testloader_9 = DataLoader(testset_9, batch_size=32, shuffle=True)\n",
    "\"\"\"## Load CNN Weights\"\"\"\n",
    "\n",
    "# save the mode weights in .pth format (99.25% accuracy\n",
    "# torch.save(model.state_dict(), 'CNN_MNSIT.pth')\n",
    "\n",
    "# NOTE: load the model weights\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(\"./CNN_MNSIT.pth\", weights_only=True))\n",
    "\n",
    "\"\"\"## Inital image setup\"\"\"\n",
    "\n",
    "img_id = 3\n",
    "input = testset_8[img_id]\n",
    "img = input[0].squeeze(0).clone()\n",
    "true_y = input[1]\n",
    "# img = transform(img)\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.savefig(f\"ID {img_id}-Digit {input[1]} original_image.png\")\n",
    "print(\n",
    "    f\"ID: {img_id}, True y = {input[1]}, probability: {F.softmax(model(input[0].unsqueeze(0)), dim=1).max():.5f}\"\n",
    ")\n",
    "print(\n",
    "    f\"predicted probability:{F.softmax(model(input[0].unsqueeze(0)), dim=1).max():.5f}\"\n",
    ")\n",
    "print(f\"pixel from {img.max()} to {img.min()}\")\n",
    "# plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20009adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION: Model definition\n",
    "\n",
    "min_val = img.min()\n",
    "max_val = img.max()\n",
    "\n",
    "\n",
    "class CustomTanh(nn.Module):\n",
    "    def __init__(self, min_val, max_val):\n",
    "        super(CustomTanh, self).__init__()\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (torch.tanh(x) + 1) * (self.max_val - self.min_val) / 2 + self.min_val\n",
    "\n",
    "\n",
    "class generator(nn.Module):\n",
    "    def __init__(self, channels_img):\n",
    "        super().__init__()\n",
    "        self.channels_img = channels_img\n",
    "        # self.features_d = features_d\n",
    "        self.k = 2\n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            # NOTE: convTranspose2d output = (input -1)*s -2p + k + op\n",
    "            # (14-1)*2 + 2 = img 28x28\n",
    "            nn.ConvTranspose2d(\n",
    "                channels_img, channels_img, kernel_size=2, stride=2, padding=0\n",
    "            ),\n",
    "            # nn.Tanh(),\n",
    "            CustomTanh(min_val, max_val),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, p):\n",
    "        # p = self.p_layer(z)\n",
    "        # p = F.softmax(p, dim=1)\n",
    "        # NOTE: original\n",
    "        x_recon = self.decoder(z)\n",
    "        p = p.float()\n",
    "        p_interpolate = F.interpolate(\n",
    "            p, size=(x_recon.shape[2], x_recon.shape[3]), mode=\"nearest\"\n",
    "        )\n",
    "\n",
    "        x_recon = x_recon * p_interpolate\n",
    "\n",
    "        return x_recon, p_interpolate\n",
    "\n",
    "\n",
    "class learner(nn.Module):\n",
    "    def __init__(self, channels_img):\n",
    "        super().__init__()\n",
    "        self.channels_img = channels_img\n",
    "        # self.features_d = features_d\n",
    "        self.k = 2\n",
    "        # encoder\n",
    "        # latent mean and variance\n",
    "        self.mean_layer = nn.Sequential(\n",
    "            # NOTE: conv2d output = (input + 2p -k)/s +1\n",
    "            # (28-2)/2 +1 = img 14x14\n",
    "            nn.Conv2d(channels_img, channels_img, kernel_size=2, stride=2),\n",
    "            nn.InstanceNorm2d(channels_img, affine=True),\n",
    "            # CustomTanh(min_val, max_val),\n",
    "            # nn.Tanh(),\n",
    "            # NOTE: eror will increase then drop\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )  # latent mean and variance\n",
    "\n",
    "        self.logvar_layer = nn.Sequential(\n",
    "            # NOTE: conv2d output = (input + 2p -k)/s +1\n",
    "            # (28-2)/2 +1 = img 14x14\n",
    "            nn.Conv2d(channels_img, channels_img, kernel_size=2, stride=2),\n",
    "            nn.InstanceNorm2d(channels_img, affine=True),\n",
    "            # nn.Tanh(),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        self.p_layer = nn.Sequential(\n",
    "            # NOTE: conv2d output = (input + 2p -k)/s +1\n",
    "            # (28-2)/2 +1 = img 14x14\n",
    "            nn.Conv2d(channels_img, channels_img, kernel_size=2, stride=2),\n",
    "            nn.InstanceNorm2d(channels_img, affine=True),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def reparameterization(self, mean, var, p):\n",
    "        # mean = mean.view(mean.size[0], -1)\n",
    "        # var = var.view(var.size[0], -1)\n",
    "        epsilon = torch.randn_like(var).to(device)\n",
    "        z = mean + var * epsilon\n",
    "        z = z * p\n",
    "        return z\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, log_var, p = self.mean_layer(x), self.logvar_layer(x), self.p_layer(x)\n",
    "        return mean, log_var, p\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, log_var, p = self.encode(x)\n",
    "        p = p > 0.5\n",
    "        z = self.reparameterization(mean, log_var, p)\n",
    "        return z, mean, log_var, p\n",
    "\n",
    "\n",
    "G = generator(1).to(device)\n",
    "L = learner(1).to(device)\n",
    "\n",
    "x = torch.randn(1, 1, 28, 28).to(device)\n",
    "z, mean, log_var, p = L(x)\n",
    "x_recon, p_interpolate = G(z, p)\n",
    "print(f\"mean:{mean.shape}, log_var:{log_var.shape}, x_recon:{x_recon.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d684ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loss_function(x, x_recon, mean, log_var, p):\n",
    "    p = p.float()\n",
    "    p_interpolate = F.interpolate(\n",
    "        p, size=(x_recon.shape[2], x_recon.shape[3]), mode=\"nearest\"\n",
    "    )\n",
    "    # NOTE: original loss\n",
    "    # reproduction_loss = F.mse_loss(x_recon, x)\n",
    "\n",
    "    # HACK: alternative loss function, only use the pixels that have high variance\n",
    "    reproduction_loss = (x_recon - x) ** 2\n",
    "    reproduction_loss = reproduction_loss * p_interpolate\n",
    "    reproduction_loss = reproduction_loss.mean()\n",
    "\n",
    "    # NOTE: original KLD\n",
    "    # KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    # NOTE: alternative KLD\n",
    "    KLD = -0.5 * torch.sum((1 + log_var - mean.pow(2) - log_var.exp()) * p)\n",
    "\n",
    "    return reproduction_loss + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad7efc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# SECTION: Training\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "epochs = 1000\n",
    "leaner_epochs = 10\n",
    "predicted = true_y\n",
    "# predicted = 9\n",
    "G = generator(1).to(device)\n",
    "L = learner(1).to(device)\n",
    "\n",
    "opt_G = torch.optim.Adam(G.parameters(), lr=0.005)\n",
    "opt_L = torch.optim.Adam(L.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(epochs + 1):\n",
    "    for leaner_epoch in range(leaner_epochs + 1):\n",
    "        opt_L.zero_grad()\n",
    "        x = img.clone().to(device)\n",
    "        x = x.view(1, 1, 28, 28)\n",
    "        z, mean, log_var, p = L(x)\n",
    "        x_recon, p_interpolate = G(z, p)\n",
    "        # z, mean, log_var = L(x)\n",
    "        # x_recon, p = G(x, z)\n",
    "\n",
    "        # Get the index of the max log-probability\n",
    "        model.eval()\n",
    "        critic_fake = F.softmax(model(x_recon), dim=1)[0][predicted]\n",
    "\n",
    "        loss_L = -(torch.mean(critic_fake))\n",
    "        # loss = loss_function(x, x_recon, mean, log_var) - torch.log(critic_real + 1e-5) * (\n",
    "        #     -torch.log(critic_fake + 1e-5)\n",
    "        # )\n",
    "\n",
    "        loss_L.backward()\n",
    "        opt_L.step()\n",
    "\n",
    "    opt_G.zero_grad()\n",
    "    x = img.clone().to(device)\n",
    "    x = x.view(1, 1, 28, 28)\n",
    "    z, mean, log_var, p = L(x)\n",
    "    x_recon, p_interpolate = G(z, p)\n",
    "\n",
    "    model.eval()\n",
    "    critic_fake = F.softmax(model(x_recon), dim=1)[0][predicted]\n",
    "    t1 = -torch.sum(torch.log(critic_fake + 1e-5))\n",
    "    t2 = loss_function(x, x_recon, mean, log_var, p)\n",
    "    t3 = -torch.sum(p * torch.log(p + 1e-5))\n",
    "\n",
    "    # FIXME: will black out the digit\n",
    "    # alpha = torch.sum(p)\n",
    "\n",
    "    # NOTE: original loss function\n",
    "    loss_G = t1 + t2 + t3\n",
    "\n",
    "    # NOTE: alternative loss function\n",
    "    # loss_G = torch.mean(critic_fake)\n",
    "\n",
    "    loss_G.backward(retain_graph=True)  # Retain graph for t3\n",
    "    opt_G.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"epoch: {epoch}, loss_L: {loss_L}, loss_G: {loss_G}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57714077",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(f\"x.max(): {x.max()}, x.min(): {x.min()}\")\n",
    "print(f\"x_recon.max(): {x_recon.max()}, x_recon.min(): {x_recon.min()}\")\n",
    "print(f\"mu.max(): {mean.max()}, mu.min(): {mean.min()}\")\n",
    "print(f\"log_var.max(): {log_var.max()}, log_var.min(): {log_var.min()}\")\n",
    "print(f\"prob: {F.softmax(model(x_recon.view(1, 1, 28, 28)), dim=1)}\")\n",
    "num_patches = (p[:, 0, :, :] > 0.5).sum()\n",
    "print(f\"num_patches: {num_patches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION: plot the reconstructed image\n",
    "\n",
    "# Add the directory containing helper.py to the Python path\n",
    "sys.path.append(os.path.abspath(\"/home/jack/Documents/PhD-research/XAI\"))\n",
    "\n",
    "# Import necessary functions from helper\n",
    "from helper import plot_recon_img, plot_patch_image\n",
    "\n",
    "plot_recon_img(x_recon, model, true_y, img_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3421c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION: find the n_th patch of image\n",
    "plot_patch_image(img, model, true_y, img_id, p, p_interpolate, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d17e927",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# SECTION: find the top n_th high variance pixels\n",
    "# maybe not important\n",
    "\n",
    "for n in range(5, 31, 5):\n",
    "    flat_tensor = log_var.exp().flatten()\n",
    "    top_10_indices = torch.topk(flat_tensor, n).indices\n",
    "    high_var_index = torch.zeros_like(flat_tensor, dtype=torch.bool)\n",
    "    high_var_index[top_10_indices] = True\n",
    "    high_var_index = high_var_index.view(log_var.shape[2], log_var.shape[3])\n",
    "\n",
    "    # Convert boolean tensor to float tensor\n",
    "    high_var_index = high_var_index.float()\n",
    "    high_var_index = high_var_index.unsqueeze(0).unsqueeze(\n",
    "        0\n",
    "    )  # Add batch and channel dimensions\n",
    "    # interpolation to 28x28\n",
    "    c = F.interpolate(high_var_index, size=(28, 28), mode=\"nearest\")\n",
    "    c = c.squeeze(0).view(1, 1, 28, 28)\n",
    "    new_image = x * c.view(1, 1, 28, 28)\n",
    "    x_recon_pred = torch.argmax(F.softmax(model(new_image), dim=1))\n",
    "    print(f\"When n={n}, x_recon_pred: {x_recon_pred}\")\n",
    "    plt.imshow(new_image.squeeze(0).squeeze(0).detach().numpy(), cmap=\"gray\")\n",
    "    # Add a colorbar to show the mapping from colors to values\n",
    "    plt.title(\n",
    "        f\"Digit {x_recon_pred} Surrogate model with prediction: {F.softmax(model(new_image), dim=1).max():.3f}\"\n",
    "    )\n",
    "    plt.savefig(f\"ID {img_id}-Digit {true_y} pred {x_recon_pred} with n={n}.png\")\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc922ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "epochs = 500\n",
    "leaner_epochs = 5\n",
    "predicted = true_y\n",
    "# predicted = 9\n",
    "G = generator(1).to(device)\n",
    "L = learner(1).to(device)\n",
    "\n",
    "opt_G = torch.optim.Adam(G.parameters(), lr=0.005)\n",
    "opt_L = torch.optim.Adam(L.parameters(), lr=0.005)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(testloader_8):\n",
    "    data = data.to(device)\n",
    "    target = target.to(device)\n",
    "    for epoch in range(epochs + 1):\n",
    "        for leaner_epoch in range(leaner_epochs + 1):\n",
    "            opt_L.zero_grad()\n",
    "            x = data.clone().to(device)\n",
    "            z, mean, log_var, p = L(x)\n",
    "            x_recon, p_interpolate = G(z, p)\n",
    "            # z, mean, log_var = L(x)\n",
    "            # x_recon, p = G(x, z)\n",
    "\n",
    "            # Get the index of the max log-probability\n",
    "            model.eval()\n",
    "            critic_fake = F.softmax(model(x_recon), dim=1)[0][predicted]\n",
    "\n",
    "            loss_L = -(torch.mean(critic_fake))\n",
    "            # loss = loss_function(x, x_recon, mean, log_var) - torch.log(critic_real + 1e-5) * (\n",
    "            #     -torch.log(critic_fake + 1e-5)\n",
    "            # )\n",
    "\n",
    "            loss_L.backward()\n",
    "            opt_L.step()\n",
    "\n",
    "        opt_G.zero_grad()\n",
    "        z, mean, log_var, p = L(x)\n",
    "        x_recon, p_interpolate = G(z, p)\n",
    "\n",
    "        model.eval()\n",
    "        critic_fake = F.softmax(model(x_recon), dim=1)[0][predicted]\n",
    "        t1 = -torch.sum(torch.log(critic_fake + 1e-5))\n",
    "        t2 = loss_function(x, x_recon, mean, log_var, p)\n",
    "        t3 = -torch.sum(p * torch.log(p + 1e-5))\n",
    "\n",
    "        # FIXME: will black out the digit\n",
    "        # alpha = torch.sum(p)\n",
    "\n",
    "        # NOTE: original loss function\n",
    "        loss_G = t1 + t2 + t3\n",
    "\n",
    "        # NOTE: alternative loss function\n",
    "        # loss_G = torch.mean(critic_fake)\n",
    "\n",
    "        loss_G.backward(retain_graph=True)  # Retain graph for t3\n",
    "        opt_G.step()\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"epoch: {epoch}, loss_L: {loss_L}, loss_G: {loss_G}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304586fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION: train testloader\n",
    "\n",
    "\n",
    "epochs = 500\n",
    "leaner_epochs = 10\n",
    "predicted = true_y\n",
    "# predicted = 9\n",
    "G = generator(1).to(device)\n",
    "L = learner(1).to(device)\n",
    "\n",
    "opt_G = torch.optim.Adam(G.parameters(), lr=0.005)\n",
    "opt_L = torch.optim.Adam(L.parameters(), lr=0.005)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(testloader):\n",
    "    data = data.to(device)\n",
    "    target = target.to(device)\n",
    "    for epoch in range(epochs + 1):\n",
    "        for leaner_epoch in range(leaner_epochs + 1):\n",
    "            opt_L.zero_grad()\n",
    "            x = data.clone().to(device)\n",
    "            z, mean, log_var, p = L(x)\n",
    "            x_recon, p_interpolate = G(z, p)\n",
    "\n",
    "            # Get the index of the max log-probability\n",
    "            predicted = target\n",
    "            model.eval()\n",
    "            critic_fake = F.softmax(model(x_recon), dim=1)[0][predicted]\n",
    "\n",
    "            loss_L = -(critic_fake.mean())\n",
    "\n",
    "            loss_L.backward()\n",
    "            opt_L.step()\n",
    "\n",
    "        opt_G.zero_grad()\n",
    "        z, mean, log_var, p = L(x)\n",
    "        x_recon, p_interpolate = G(z, p)\n",
    "\n",
    "        model.eval()\n",
    "        critic_fake = F.softmax(model(x_recon), dim=1)[0][predicted]\n",
    "        t1 = -torch.sum(torch.log(critic_fake + 1e-5))\n",
    "        t2 = loss_function(x, x_recon, mean, log_var, p)\n",
    "        t3 = -torch.sum(p * torch.log(p + 1e-5))\n",
    "\n",
    "        # FIXME: will black out the digit\n",
    "        # alpha = torch.sum(p)\n",
    "\n",
    "        # NOTE: original loss function\n",
    "        loss_G = t1 + t2 + t3\n",
    "\n",
    "        # NOTE: alternative loss function\n",
    "        # loss_G = torch.mean(critic_fake)\n",
    "\n",
    "        loss_G.backward(retain_graph=True)  # Retain graph for t3\n",
    "        opt_G.step()\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"epoch: {epoch}, loss_L: {loss_L}, loss_G: {loss_G}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
