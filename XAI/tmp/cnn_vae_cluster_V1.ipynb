{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac970c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: Define a CNN model for MNIST dataset and load the model weights\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "import torch.distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Define the CNN model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = torch.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "trainset = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd588988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MNIST_8(Dataset):\n",
    "    def __init__(self, mnist_dataset):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.eight_indices = [\n",
    "            i for i, (img, label) in enumerate(self.mnist_dataset) if label == 8\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.mnist_dataset[self.eight_indices[index]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.eight_indices)\n",
    "\n",
    "\n",
    "# Create the dataset for digit 8\n",
    "testset_8 = MNIST_8(testset)\n",
    "testloader_8 = DataLoader(testset_8, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\"\"\"## Load CNN Weights\"\"\"\n",
    "\n",
    "# save the mode weights in .pth format (99.25% accuracy\n",
    "# torch.save(model.state_dict(), 'CNN_MNSIT.pth')\n",
    "\n",
    "# NOTE: load the model weights\n",
    "\n",
    "model.load_state_dict(torch.load(\"./CNN_MNSIT.pth\", weights_only=True))\n",
    "\n",
    "\"\"\"## Inital image setup\"\"\"\n",
    "\n",
    "img_id = 3\n",
    "input = testset_8[img_id].to(device)\n",
    "img = input[0].squeeze(0).clone()\n",
    "true_y = input[1]\n",
    "# img = transform(img)\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.savefig(f\"ID {img_id}-Digit {input[1]} original_image.png\")\n",
    "print(\n",
    "    f\"ID: {img_id}, True y = {input[1]}, probability: {F.softmax(model(input[0].unsqueeze(0)), dim=1).max():.5f}\"\n",
    ")\n",
    "print(\n",
    "    f\"predicted probability:{F.softmax(model(input[0].unsqueeze(0)), dim=1).max():.5f}\"\n",
    ")\n",
    "print(f\"pixel from {img.max()} to {img.min()}\")\n",
    "# plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eabeeb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class VI(nn.Module):\n",
    "    def __init__(self, dim, K=2):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.q_dim = dim\n",
    "        h1_dim = 20\n",
    "        h2_dim = 10\n",
    "\n",
    "        self.q_c = nn.Sequential(\n",
    "            nn.Linear(self.q_dim, h1_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1_dim, h2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2_dim, h2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2_dim, self.K * self.q_dim),\n",
    "        )\n",
    "        self.q_mu = nn.Sequential(\n",
    "            nn.Linear(self.q_dim, h1_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1_dim, h2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2_dim, self.K),\n",
    "            # nn.Linear(h2_dim, self.q_dim),\n",
    "        )\n",
    "        self.q_log_var = nn.Sequential(\n",
    "            nn.Linear(self.q_dim, h1_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1_dim, h2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2_dim, self.K),\n",
    "            # nn.Linear(h2_dim, self.q_dim),\n",
    "        )\n",
    "        self.mu_y = nn.Sequential(\n",
    "            nn.Linear(self.q_dim, h1_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1_dim, h2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2_dim, self.q_dim),\n",
    "            # nn.Linear(h2_dim, 1),\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var, phi):\n",
    "        # std can not be negative, thats why we use log variance\n",
    "        sigma = torch.exp(0.5 * log_var) + 1e-5\n",
    "        sigma = sigma.unsqueeze(0)\n",
    "        mu = mu.unsqueeze(0)\n",
    "        eps = torch.randn_like(phi)\n",
    "        z = mu + sigma * eps\n",
    "        z = z * phi\n",
    "        # return z.sum(dim=1) + 10\n",
    "        return z.sum(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        phi = self.q_c(x) ** 2\n",
    "        phi = phi.view(self.q_dim, self.K)\n",
    "        # NOTE: softmax winner takes all\n",
    "        phi = F.softmax(phi, dim=1)\n",
    "\n",
    "        # phi = phi / phi.sum(dim=1).view(-1, 1)\n",
    "\n",
    "        mu = self.q_mu(x)\n",
    "        log_var = self.q_log_var(x)\n",
    "        z = self.reparameterize(mu, log_var, phi)\n",
    "        mu_y = self.mu_y(z)\n",
    "        return z, mu, log_var, phi, mu_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3515959",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loss_elbo(x, mu, log_var, phi, x_recon, model, predicted):\n",
    "    # HACK: use the CNN model predition as the input\n",
    "    # log_var = log_var + 1e-5\n",
    "    phi = phi + 1e-10\n",
    "    high_mu_index = mu.argmax()\n",
    "    high_phi_index = phi[:, high_mu_index] > 0.5\n",
    "    lamb = (high_phi_index > 0.5).sum()\n",
    "\n",
    "    t1 = -0.5 * (mu.view(1, -1) - mu_y.view(-1, 1)) ** 2\n",
    "    t1 = phi * t1\n",
    "    # t1 = t1.mean()\n",
    "    t1 = t1.sum()\n",
    "\n",
    "    # t1 = torch.outer(mu_y, mu) - 0.5 * mu.view(1, -1) **2\n",
    "    # t1 = -0.5  * (log_var.exp() + mu**2).view(1, -1) + t1\n",
    "    # t1 = phi * t1\n",
    "    # t1 = t1.sum()\n",
    "\n",
    "    # NOTE: Alternative implementation\n",
    "    # t2 = 0.5 * (x - x_recon) ** 2\n",
    "    # t2 = -torch.mean(t2)\n",
    "    # t2_1 = 0.5 * (x - x_recon) ** 2\n",
    "    # t2_1 = -torch.mean(t2_1[high_mu_index])\n",
    "\n",
    "    # NOTE: this is correct\n",
    "    t2 = torch.outer(x, mu) - 0.5 * x.view(-1, 1) ** 2\n",
    "    t2 = -0.5 * (log_var.exp() + mu**2).view(1, -1) + t2\n",
    "    t2 = phi * t2\n",
    "    # t2 = torch.mean(t2)\n",
    "    t2 = torch.sum(t2)\n",
    "\n",
    "    # t3 = -torch.log(phi).mean()\n",
    "    t3 = phi * torch.log(phi)\n",
    "    t3 = -torch.sum(t3)\n",
    "\n",
    "    # t4 = 0.5 * log_var.mean()\n",
    "    t4 = torch.pi * log_var.sum()\n",
    "\n",
    "    # HACK: use the CNN model predition as the input\n",
    "    # x_recon = x_recon - 10\n",
    "    model.eval()\n",
    "    input = x_recon.view(1, 1, 28, 28)\n",
    "    # Forward pass\n",
    "    outputs = model(input)\n",
    "    outputs = F.softmax(outputs, dim=1)\n",
    "    outputs = torch.clamp(outputs, 1e-5, 1 - 1e-5)\n",
    "    t5 = torch.log(outputs[:, predicted])\n",
    "    # print(f't1: {t1}, t2: {t2}, t3: {t3}, t4: {t4}, t5: {t5}, lamb: {lamb}')\n",
    "    return -(t1 + t2 + t3 + t4 - t5)\n",
    "    # return (t1 + t2  + t3 + t4) * (t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd6000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ll_gaussian(x, mu, log_var):\n",
    "    sigma = torch.exp(0.5 * log_var)\n",
    "    return -0.5 * torch.log(2 * np.pi * sigma**2) - (1 / (2 * sigma**2)) * (x - mu) ** 2\n",
    "\n",
    "\n",
    "def loss_gau_elbo(x, mu, log_var, phi, x_recon, model, predicted):\n",
    "    phi = phi + 1e-10\n",
    "    t1 = ll_gaussian(x_recon, torch.tensor(0), torch.tensor(0))\n",
    "    t1 = phi * t1\n",
    "    t1 = torch.mean(t1)\n",
    "\n",
    "    t2 = ll_gaussian(x, x_recon, torch.tensor(0))\n",
    "    t2 = phi * t2\n",
    "    t2 = torch.mean(t2)\n",
    "\n",
    "    t3 = phi * torch.log(phi)\n",
    "    t3 = -torch.mean(t3)\n",
    "\n",
    "    t4 = ll_gaussian(x_recon, mu, log_var)\n",
    "    t4 = torch.mean(t4)\n",
    "    # HACK: use the CNN model predition as the input\n",
    "    # x_recon = x_recon - 10\n",
    "    model.eval()\n",
    "    input = x_recon.view(1, 1, 28, 28)\n",
    "    # Forward pass\n",
    "    outputs = model(input)\n",
    "    outputs = F.softmax(outputs, dim=1)\n",
    "    t5 = torch.log(outputs[:, predicted] + 1e-10)\n",
    "    # print(f\"t1: {t1}, t2: {t2}, t3: {t3}, t4: {t4}, t5: {t5}\")\n",
    "    return -(t1 + t2 + t3 + t4 - t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db69a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cluster_elbo(x, mu, log_var, phi, x_recon, model, predicted):\n",
    "    lamb = 25\n",
    "    phi = phi + 1e-5\n",
    "    x1 = x.view(-1, 1) * phi\n",
    "    x1 = x1.sum(dim=1)\n",
    "\n",
    "    t1 = (x - x_recon) ** 2\n",
    "    t1 = -torch.mean(t1)\n",
    "\n",
    "    # t2 = log_var.exp().view(-1, 1) + x_recon.view(1,-1) ** 2\n",
    "    # t2 = log_var.exp().view(-1, 1)\n",
    "    # t2 = phi * t2\n",
    "    # t2 = torch.mean(t2)\n",
    "\n",
    "    t3 = phi * torch.log(phi)\n",
    "    t3 = -torch.mean(t3)\n",
    "    model.eval()\n",
    "    # input = x_recon.view(1, 1, 28, 28)\n",
    "    input = x1.view(1, 1, 28, 28)\n",
    "    # Forward pass\n",
    "    outputs = model(input)\n",
    "    outputs = F.softmax(outputs, dim=1)\n",
    "    outputs = torch.clamp(outputs, 1e-5, 1 - 1e-5)\n",
    "    t5 = torch.log(outputs[:, predicted])\n",
    "\n",
    "    # return -(t1 * lamb) * (-t5)\n",
    "    return (-t5) + t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09a30cd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "mu = None\n",
    "log_var = None\n",
    "predicted = true_y\n",
    "q_dim = 784\n",
    "epochs = 5000\n",
    "m = VI(q_dim, 10).to(device)\n",
    "optim = torch.optim.Adam(m.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(epochs + 1):\n",
    "    x = img.view(784).clone().to(device)\n",
    "    optim.zero_grad()\n",
    "    x_recon, mu, log_var, phi, mu_y = m(x)\n",
    "    # Get the index of the max log-probability\n",
    "\n",
    "    loss = loss_elbo(x, mu, log_var, phi, x_recon, model, predicted)\n",
    "    # loss = loss_gau_elbo(x, mu, log_var, phi, x_recon, model, predicted)\n",
    "    # loss = loss_cluster_elbo(x, mu, log_var, phi, x_recon, model, predicted)\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"epoch: {epoch}, loss: {loss}\")\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "    # loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "print(f\"x.max(): {x.max()}, x.min(): {x.min()}\")\n",
    "print(f\"mu.max(): {mu.max()}, mu.min(): {mu.min()}\")\n",
    "print(f\"mu_y.max(): {mu_y.max()}, mu_y.min(): {mu_y.min()}\")\n",
    "print(f\"var.max(): {log_var.exp().max()}, var.min(): {log_var.exp().min()}\")\n",
    "print(f\"prob: {F.softmax(model(x_recon.view(1, 1, 28, 28)), dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a903b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "new_image = x_recon.view(1, 1, 28, 28)\n",
    "# new_image = mu_y.view(1, 1, 28, 28)\n",
    "x_recon_pred = torch.argmax(F.softmax(model(new_image), dim=1))\n",
    "print(\n",
    "    f\"True y = {true_y}. New image full model prediction: {F.softmax(model(new_image))}\"\n",
    ")\n",
    "plt.imshow(new_image.squeeze(0).squeeze(0).detach().numpy(), cmap=\"gray\")\n",
    "plt.title(\n",
    "    f\"Digit {x_recon_pred} Surrogate model with prediction: {F.softmax(model(new_image), dim=1).max():.3f}\"\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.savefig(f\"ID {img_id}-Digit {true_y} pred {x_recon_pred} new_image.png\")\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56bafc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The model in VI evaluation\n",
    "k = 0.5\n",
    "high_phi_index = np.where(phi[:, mu.argmax()].view(28, 28) > k)\n",
    "\n",
    "print(f\"number of high_phi_index:{high_phi_index[0].size}\")\n",
    "\n",
    "plt.imshow(img.clone().detach().numpy(), cmap=\"gray\")\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "# Scatter plot with colors corresponding to exp_values\n",
    "plt.scatter(\n",
    "    # high_var_index[1], high_var_index[0], s=10, c=exp_values_flatten, cmap=\"viridis\"\n",
    "    high_phi_index[1],\n",
    "    high_phi_index[0],\n",
    "    s=10,\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "\n",
    "# Add a colorbar to show the mapping from colors to values\n",
    "plt.title(\n",
    "    f\"Digit {x_recon_pred} Surrogate model with prediction: {F.softmax(model(new_image), dim=1).max():.3f}\"\n",
    ")\n",
    "plt.savefig(\n",
    "    f\"ID {img_id}-Digit {true_y} pred {x_recon_pred} with {epochs} epochs({k}k).png\"\n",
    ")\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a7537",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: THe cluster in VI evaluation\n",
    "clusters = phi.argmax(1).view(28, 28)\n",
    "plt.imshow(clusters.clone().detach().numpy(), cmap=\"viridis\")\n",
    "\n",
    "\n",
    "plt.savefig(\n",
    "    f\"Cluster-ID {img_id}-Digit {true_y} pred {x_recon_pred} with {epochs} epochs({k}k).png\"\n",
    ")\n",
    "print(f\"mu: {mu}\")\n",
    "\n",
    "\n",
    "def img_mask_func(img, mu, true_y=true_y, x_recon_pred=x_recon_pred):\n",
    "    for i in range(mu.shape.__getitem__(0)):\n",
    "        mask_img = img.clone().detach()\n",
    "        mask_img = (clusters == i) * mask_img\n",
    "        model.eval()\n",
    "        output = F.softmax(model(mask_img.view(1, 1, 28, 28)), dim=1)\n",
    "        print(f\"prob: {F.softmax(model(mask_img.view(1, 1, 28, 28)), dim=1)}\")\n",
    "        if output.max() > 0.5:\n",
    "            print(f\"Cluster {i} has a prediction\")\n",
    "            plt.imshow(mask_img.squeeze(0).squeeze(0).detach().numpy(), cmap=\"gray\")\n",
    "            plt.savefig(\n",
    "                f\"ID {img_id}-Digit {true_y} pred {x_recon_pred} cluster {i} mask_image.png\"\n",
    "            )\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "\n",
    "\n",
    "img_mask_func(img, mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_img = img.clone().detach()\n",
    "\n",
    "\n",
    "# mask_img[i, j] = img.min()\n",
    "def img_mask_func(img, high_phi_index):\n",
    "    for k in range(high_phi_index[0].size):\n",
    "        for i in range(0, 1, 1):\n",
    "            for j in range(0, 1, 1):\n",
    "                mask_img[high_phi_index[0][k] + i, high_phi_index[1][k] + j] = img.min()\n",
    "    return img\n",
    "\n",
    "\n",
    "mask_img = img_mask_func(mask_img, high_phi_index)\n",
    "# mask_img[high_phi_index] = img.min()\n",
    "print(f\"prob: {F.softmax(model(mask_img.view(1, 1, 28, 28)), dim=1)}\")\n",
    "plt.imshow(mask_img.squeeze(0).squeeze(0).detach().numpy(), cmap=\"gray\")\n",
    "plt.savefig(f\"ID {img_id}-Digit {true_y} mask_image.png\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
