{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "hxhXs7yxKu"
      },
      "source": [
        "# Hamiltonian Monte Carlo (HMC) Algorithm\n",
        "\n",
        "Hamiltonian Monte Carlo (HMC) is a Markov Chain Monte Carlo (MCMC) method that uses Hamiltonian dynamics to propose moves in a way that explores the target distribution more efficiently than random-walk-based methods like Metropolis-Hastings.\n",
        "\n",
        "The core idea is to introduce auxiliary momentum variables and simulate a system of particles evolving according to the laws of Hamiltonian dynamics. The positions correspond to the target distribution's parameters, and the momenta are auxiliary variables used to facilitate exploration.\n",
        "\n",
        "## Key Components of the HMC Algorithm\n",
        "\n",
        "1. **Hamiltonian Dynamics**: The system's evolution is governed by the Hamiltonian, which is the sum of potential energy (related to the target distribution) and kinetic energy (related to the auxiliary momentum variables).\n",
        "2. **Momentum Variables**: These are introduced to help explore the state space efficiently.\n",
        "3. **Leapfrog Integration**: This is a numerical method to simulate the continuous dynamics of the system using discrete steps.\n",
        "\n",
        "### Hamiltonian Function\n",
        "\n",
        "The Hamiltonian function $H(x, p)$ is the sum of the **potential energy** $U(x)$ and the **kinetic energy** $K(p)$:\n",
        "\n",
        "$$\n",
        "H(x, p) = U(x) + K(p)\n",
        "$$\n",
        "\n",
        "- $U(x)$ is the potential energy, which corresponds to the negative log of the target distribution. If the target distribution is $p(x)$, then:\n",
        "\n",
        "$$\n",
        "U(x) = -\\log(p(x))\n",
        "$$\n",
        "\n",
        "- $K(p)$ is the kinetic energy, which is typically chosen to be a Gaussian distribution over the momentum variables $p$. For simplicity, we often assume the mass matrix $M = I$, the identity matrix. Hence, the kinetic energy is:\n",
        "\n",
        "$$\n",
        "K(p) = \\frac{1}{2} p^T M^{-1} p\n",
        "$$\n",
        "\n",
        "For a unit mass matrix, $M^{-1} = I$, and we have:\n",
        "\n",
        "$$\n",
        "K(p) = \\frac{1}{2} p^T p\n",
        "$$\n",
        "\n",
        "Thus, the Hamiltonian becomes:\n",
        "\n",
        "$$\n",
        "H(x, p) = -\\log(p(x)) + \\frac{1}{2} p^T p\n",
        "$$\n",
        "\n",
        "### Equations of Motion\n",
        "\n",
        "Hamilton\u2019s equations describe how the position $x$ and momentum $p$ evolve over time. These equations are derived from the Hamiltonian:\n",
        "\n",
        "1. **For position $x$**, the rate of change is the gradient of the Hamiltonian with respect to the momentum $p$:\n",
        "\n",
        "$$\n",
        "\\frac{dx}{dt} = \\frac{\\partial H(x, p)}{\\partial p} = M^{-1} p\n",
        "$$\n",
        "\n",
        "2. **For momentum $p$**, the rate of change is the negative gradient of the Hamiltonian with respect to position $x$:\n",
        "\n",
        "$$\n",
        "\\frac{dp}{dt} = -\\frac{\\partial H(x, p)}{\\partial x} = -\\nabla U(x)\n",
        "$$\n",
        "\n",
        "These equations describe the continuous time dynamics of the system.\n",
        "\n",
        "### Leapfrog Integration\n",
        "\n",
        "To simulate the continuous Hamiltonian dynamics discretely, we use the **leapfrog integration method**. The leapfrog algorithm alternates between updating the momentum and position over small time steps.\n",
        "\n",
        "1. **Half-step update of momentum**:\n",
        "\n",
        "$$\n",
        "p_{\\frac{1}{2}} = p - \\frac{\\epsilon}{2} \\nabla U(x)\n",
        "$$\n",
        "\n",
        "2. **Full-step update of position**:\n",
        "\n",
        "$$\n",
        "x' = x + \\epsilon M^{-1} p_{\\frac{1}{2}}\n",
        "$$\n",
        "\n",
        "3. **Half-step update of momentum**:\n",
        "\n",
        "$$\n",
        "p' = p_{\\frac{1}{2}} - \\frac{\\epsilon}{2} \\nabla U(x')\n",
        "$$\n",
        "\n",
        "Here, $\\epsilon$ is the step size, and $M^{-1}$ is the inverse mass matrix (usually the identity matrix for simplicity).\n",
        "\n",
        "### HMC Algorithm\n",
        "\n",
        "The full Hamiltonian Monte Carlo algorithm proceeds as follows:\n",
        "\n",
        "1. **Initialization**: Start with an initial position $x_0$.\n",
        "2. **Momentum Sampling**: Sample an initial momentum $p_0 \\sim \\mathcal{N}(0, M)$, typically a Gaussian distribution with mean 0 and covariance $M$.\n",
        "3. **Leapfrog Integration**: Perform $L$ leapfrog steps to simulate the Hamiltonian dynamics and propose a new state $(x', p')$.\n",
        "4. **Metropolis Acceptance Step**: Accept the proposed state with probability:\n",
        "\n",
        "$$\n",
        "\\alpha = \\min \\left( 1, \\exp \\left( -H(x', p') + H(x_0, p_0) \\right) \\right)\n",
        "$$\n",
        "\n",
        "If the move is accepted, set $x_0 = x'$. If rejected, keep $x_0$ unchanged.\n",
        "\n",
        "### Full Algorithm Steps\n",
        "\n",
        "```python\n",
        "# HMC Algorithm\n",
        "1. Initialize x_0\n",
        "2. For iteration in 1 to N:\n",
        "   1. Sample momentum p_0 ~ N(0, M)\n",
        "   2. Simulate dynamics using leapfrog for L steps\n",
        "   3. Propose new state (x', p')\n",
        "   4. Calculate acceptance probability:\n",
        "      \u03b1 = min(1, exp(-H(x', p') + H(x_0, p_0)))\n",
        "   5. Accept or reject the new state based on \u03b1\n",
        "```\n",
        "\n",
        "## Derivations\n",
        "\n",
        "### Hamiltonian Function\n",
        "\n",
        "The Hamiltonian is given by:\n",
        "\n",
        "$$\n",
        "H(x, p) = U(x) + K(p)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $U(x) = -\\log(p(x))$ (the potential energy based on the target distribution).\n",
        "- $K(p) = \\frac{1}{2} p^T M^{-1} p$ (the kinetic energy, assuming unit mass).\n",
        "\n",
        "### Equations of Motion\n",
        "\n",
        "Hamilton\u2019s equations describe the time evolution of position and momentum:\n",
        "\n",
        "1. **For position $x$**:\n",
        "\n",
        "$$\n",
        "\\frac{dx}{dt} = \\frac{\\partial H}{\\partial p} = M^{-1} p\n",
        "$$\n",
        "\n",
        "2. **For momentum $p$**:\n",
        "\n",
        "$$\n",
        "\\frac{dp}{dt} = -\\frac{\\partial H}{\\partial x} = -\\nabla U(x)\n",
        "$$\n",
        "\n",
        "### Leapfrog Integration\n",
        "\n",
        "The leapfrog method involves three main steps:\n",
        "\n",
        "1. **Half-step update for momentum**:\n",
        "\n",
        "$$\n",
        "p_{\\frac{1}{2}} = p - \\frac{\\epsilon}{2} \\nabla U(x)\n",
        "$$\n",
        "\n",
        "2. **Full-step update for position**:\n",
        "\n",
        "$$\n",
        "x' = x + \\epsilon M^{-1} p_{\\frac{1}{2}}\n",
        "$$\n",
        "\n",
        "3. **Half-step update for momentum**:\n",
        "\n",
        "$$\n",
        "p' = p_{\\frac{1}{2}} - \\frac{\\epsilon}{2} \\nabla U(x')\n",
        "$$\n",
        "\n",
        "### Metropolis Acceptance Probability\n",
        "\n",
        "The acceptance criterion ensures the detailed balance of the Markov Chain. The acceptance probability is given by:\n",
        "\n",
        "$$\n",
        "\\alpha = \\min \\left( 1, \\exp \\left( -H(x', p') + H(x_0, p_0) \\right) \\right)\n",
        "$$\n",
        "\n",
        "This step ensures that the trajectory is reversible and that the algorithm samples from the correct distribution.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Hamiltonian Monte Carlo is a powerful technique for efficiently sampling from complex, high-dimensional distributions. By leveraging the dynamics of Hamiltonian mechanics, HMC avoids the random-walk behavior typical of simpler MCMC methods and allows for more efficient exploration of the target distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "woyvkj0Uli"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Target distribution: Unnormalized log-probability (negative potential energy)\n",
        "def log_target_distribution(x):\n",
        "    return -0.5 * x**2  # Example: Standard Gaussian distribution\n",
        "\n",
        "\n",
        "# Gradient of the log-target distribution (negative gradient of potential energy)\n",
        "def grad_log_target_distribution(x):\n",
        "    return -x  # Derivative of -0.5 * x^2 is -x\n",
        "\n",
        "\n",
        "# Hamiltonian Monte Carlo (HMC) function\n",
        "def hmc(\n",
        "    log_target, grad_log_target, initial_position, step_size, num_steps, num_samples\n",
        "):\n",
        "    position = initial_position\n",
        "    samples = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # Sample momentum from a standard Gaussian distribution\n",
        "        momentum = np.random.normal(0, 1)\n",
        "\n",
        "        # Initial energy (Hamiltonian)\n",
        "        current_U = -log_target(position)\n",
        "        current_K = 0.5 * momentum**2\n",
        "        current_H = current_U + current_K\n",
        "\n",
        "        # Leapfrog integration\n",
        "        new_position, new_momentum = position, momentum\n",
        "        for _ in range(num_steps):\n",
        "            # Half-step update for momentum\n",
        "            new_momentum -= 0.5 * step_size * grad_log_target(new_position)\n",
        "            # Full-step update for position\n",
        "            new_position += step_size * new_momentum\n",
        "            # Half-step update for momentum\n",
        "            new_momentum -= 0.5 * step_size * grad_log_target(new_position)\n",
        "\n",
        "        # Proposed energy (Hamiltonian)\n",
        "        proposed_U = -log_target(new_position)\n",
        "        proposed_K = 0.5 * new_momentum**2\n",
        "        proposed_H = proposed_U + proposed_K\n",
        "\n",
        "        # Metropolis-Hastings acceptance step\n",
        "        if np.random.rand() < np.exp(current_H - proposed_H):\n",
        "            position = new_position  # Accept proposal\n",
        "        samples.append(position)\n",
        "\n",
        "    return np.array(samples)\n",
        "\n",
        "\n",
        "# Parameters\n",
        "initial_position = 0.0  # Starting point\n",
        "step_size = 0.1  # Leapfrog step size\n",
        "num_steps = 10  # Number of Leapfrog steps per iteration\n",
        "num_samples = 1000  # Number of samples to generate\n",
        "\n",
        "# Run HMC\n",
        "samples = hmc(\n",
        "    log_target_distribution,\n",
        "    grad_log_target_distribution,\n",
        "    initial_position,\n",
        "    step_size,\n",
        "    num_steps,\n",
        "    num_samples,\n",
        ")\n",
        "\n",
        "# Plot results\n",
        "plt.hist(samples, bins=30, density=True, alpha=0.5, label=\"HMC Samples\")\n",
        "x = np.linspace(-5, 5, 1000)\n",
        "plt.plot(\n",
        "    x,\n",
        "    np.exp(log_target_distribution(x)) / np.sqrt(2 * np.pi),\n",
        "    label=\"True Distribution\",\n",
        ")\n",
        "plt.title(\"HMC Sampling from a Gaussian Distribution\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}